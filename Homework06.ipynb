{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework06\n",
    "\n",
    "Exercises to practice pandas, data analysis and regression\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand the effects of pre-processing data\n",
    "- Get familiar with the ML flow: encode -> normalize -> train -> evaluate\n",
    "- Understand the difference between regression and classification tasks\n",
    "- Build an intuition for different regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from data_utils import object_from_json_url\n",
    "from data_utils import StandardScaler\n",
    "from data_utils import LinearRegression, SGDRegressor\n",
    "from data_utils import regression_error\n",
    "\n",
    "from data_utils import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Let's load up the full [ANSUR](https://www.openlab.psu.edu/ansur2/) dataset that we looked at briefly in [Week 02](https://github.com/DM-GY-9103-2024F-H/WK02).\n",
    "\n",
    "This is the dataset that has anthropometric information about U.S. Army personnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "ANSUR_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/main/datasets/json/ansur.json\"\n",
    "ansur_data = object_from_json_url(ANSUR_FILE)\n",
    "\n",
    "# Look at first 2 records\n",
    "ansur_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested data\n",
    "\n",
    "This is that *nested* dataset from Week 02.\n",
    "\n",
    "# ü§î\n",
    "\n",
    "Let's load it into a `DataFrame` to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into DataFrame\n",
    "ansur_df = pd.DataFrame.from_records(ansur_data)\n",
    "ansur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# üòìüôÑ\n",
    "\n",
    "That didn't work too well. We ended up with objects in our columns.\n",
    "\n",
    "Luckily, our `DataFrame` library has a function called [`json_normalize()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) that can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into DataFrame\n",
    "ansur_df = pd.json_normalize(ansur_data)\n",
    "ansur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. `DataFrames` are magic.\n",
    "\n",
    "#### Data Exploration\n",
    "\n",
    "Before we start creating models, let's do a little bit of data analysis and get a feeling for the shapes, distributions and relationships of our data.\n",
    "\n",
    "1. Print `min`, `max` and `average` values for all of the features.\n",
    "2. Print `covariance` tables for `age`, `ear.length` and `head.circumference`.\n",
    "3. Plot `age`, `ear.length` and `head.circumference` versus the $1$ *feature* that is most correlated to each of them.\n",
    "\n",
    "Don't forget to *encode* and *normalize* the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Work on Data Exploration here\n",
    "# print(\"raw data describe:\")\n",
    "# display(ansur_df.describe())\n",
    "\n",
    "### Encode non-numerical features\n",
    "\n",
    "# need to encode \"gender\" col\n",
    "genders = ['M', 'F']\n",
    "print(genders)\n",
    "gender_encoder = OrdinalEncoder(categories=[genders])\n",
    "gender_vals = gender_encoder.fit_transform(ansur_df[[\"gender\"]].values)\n",
    "ansur_df[[\"gender\"]] = gender_vals # gender is now 0 or 1\n",
    "\n",
    "\n",
    "## 1. Print min, max, avg\n",
    "print(\"encoded data describe:\")\n",
    "display(ansur_df.describe())\n",
    "\n",
    "### Normalize all data\n",
    "stdScaler = StandardScaler()\n",
    "ansur_scaled = stdScaler.fit_transform(ansur_df)\n",
    "print(\"normalized data describe:\")\n",
    "display(ansur_scaled.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Print Covariances\n",
    "cov_age = ansur_scaled.cov()[\"age\"]\n",
    "print(\"covariance age:\")\n",
    "display(cov_age.abs().sort_values())\n",
    "\n",
    "cov_earL = ansur_scaled.cov()[\"ear.length\"]\n",
    "print(\"covariance ear length:\")\n",
    "display(cov_earL.abs().sort_values())\n",
    "\n",
    "cov_headC = ansur_scaled.cov()[\"head.circumference\"]\n",
    "print(\"covariance head circumference:\")\n",
    "display(cov_headC.abs().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Plot features most correlated to age, ear length and head circumference\n",
    "ages = ansur_scaled[\"age\"]\n",
    "ear_Ls = ansur_scaled[\"ear.length\"]\n",
    "head_Cs = ansur_scaled[\"head.circumference\"]\n",
    "\n",
    "# age max cov: ear.length = 0.292098\n",
    "plt.scatter(ear_Ls, ages, marker='o', linestyle='', alpha=0.3)\n",
    "plt.xlabel(\"ear length\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.show()\n",
    "\n",
    "# ear length max cov: weight = 0.487481\n",
    "weights = ansur_scaled[\"weight\"]\n",
    "plt.scatter(weights, ear_Ls,  marker='o', linestyle='', alpha=0.3)\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"ear length\")\n",
    "plt.show()\n",
    "\n",
    "# head circumference max cov: head.height = 0.546534\n",
    "head_Hs = ansur_scaled[\"head.height\"]\n",
    "plt.scatter(head_Hs, head_Cs, marker='o', linestyle='', alpha=0.3)\n",
    "plt.xlabel(\"head height\")\n",
    "plt.ylabel(\"head circumference\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Does anything stand out about these graphs? Or the correlations?<br>\n",
    "Are correlations symmetric? Does the feature most correlated to ear length also have ear length as its most correlated pair?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "The graphs for ear length and head circumerference with their most correlated features seem to be a relatively linear upward trend. The graph for age however doesn't look as clear, and was also the lowest covariance number.\n",
    "\n",
    "The 2 graphs that include ear length (age and ear length) seem to have discrete measurements, whereas the graph for head circumference is more clustered without discrete values. \n",
    "\n",
    "The feature most correlated to age is ear length, but the feature most correlated to ear length is weight, so they are not symmetrical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Now, we want to create a regression model to predict `head.circumference` from the data.\n",
    "\n",
    "From our [Week 06](https://github.com/PSAM-5020-2025S-A/WK06) notebook, we can create a regression model by following these steps:\n",
    "\n",
    "1. Load dataset (done! üéâ)\n",
    "2. Encode label features as numbers (done! ‚ö°Ô∏è)\n",
    "3. Normalize the data (done! üçæ)\n",
    "4. Separate the outcome variable and the input features\n",
    "5. Create a regression model using all features\n",
    "6. Run model on training data and measure error\n",
    "7. Plot predictions and interpret results\n",
    "8. Run model on test data, measure error, plot predictions, interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Work on Regression Model here\n",
    "\n",
    "## Separate outcome variable and input features\n",
    "\n",
    "# outcome variable: \n",
    "headCs = ansur_scaled[\"head.circumference\"]\n",
    "features = ansur_scaled.drop(columns=[\"head.circumference\"])\n",
    "\n",
    "# create extra features to improve the model\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "features_poly = poly.fit_transform(features)\n",
    "\n",
    "## Create a regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# start with linear regression for all features\n",
    "# model.fit(features, headCs)\n",
    "\n",
    "# see if model with poly features is better\n",
    "model.fit(features_poly, headCs)\n",
    "\n",
    "## Measure error on training data\n",
    "# predicted_scaled = model.predict(features)\n",
    "predicted_scaled = model.predict(features_poly)\n",
    "predicted = stdScaler.inverse_transform(predicted_scaled)\n",
    "\n",
    "regression_error(predicted, ansur_df[\"head.circumference\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by plotting the features that were most closely related to head circumference\n",
    "\n",
    "for feature in [\"head.height\", \"weight\", \"foot.length\"]:\n",
    "    feat = ansur_df[feature]\n",
    "    plt.plot(feat, ansur_df[\"head.circumference\"], marker='o', linestyle='', alpha=0.3)\n",
    "    plt.plot(feat, predicted[\"head.circumference\"], color='r', marker='o', linestyle='', alpha=0.3)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"head.circumference\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Test Data\n",
    "ANSUR_TEST_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/main/datasets/json/ansur-test.json\"\n",
    "\n",
    "ansur_test_data = object_from_json_url(ANSUR_TEST_FILE)\n",
    "ansur_test_df = pd.json_normalize(ansur_test_data)\n",
    "\n",
    "ansur_test_encoded_df = ansur_test_df.copy()\n",
    "\n",
    "g_vals = gender_encoder.transform(ansur_test_df[[\"gender\"]].values)\n",
    "ansur_test_encoded_df[[\"gender\"]] = g_vals\n",
    "\n",
    "ansur_test_scaled_df = stdScaler.transform(ansur_test_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Run model on test data\n",
    "\n",
    "headCs_test = ansur_test_scaled_df[\"head.circumference\"]\n",
    "features_test = ansur_test_scaled_df.drop(columns=[\"head.circumference\"])\n",
    "# poly_test = PolynomialFeatures(degree=3, include_bias=False)\n",
    "# features_poly_test = poly_test.fit_transform(features_test)\n",
    "\n",
    "model.fit(features_test, headCs_test)\n",
    "\n",
    "## Measure error on test data\n",
    "predicted_scaled_test = model.predict(features_test)\n",
    "predicted_test = stdScaler.inverse_transform(predicted_scaled_test)\n",
    "\n",
    "display(regression_error(predicted_test, ansur_test_df[\"head.circumference\"]))\n",
    "\n",
    "## Plot predictions and interpret results\n",
    "\n",
    "for feature in [\"head.height\", \"weight\", \"foot.length\"]:\n",
    "    feat = ansur_test_df[feature]\n",
    "    plt.plot(feat, ansur_test_df[\"head.circumference\"], color='b', marker='o', linestyle='', alpha=0.3)\n",
    "    plt.plot(feat, predicted_test[\"head.circumference\"], color='r', marker='o', linestyle='', alpha=0.3)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"head.circumference\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How well does your model perform?<br>\n",
    "How could you improve it?<br>\n",
    "Are there ranges of circumferences that don't get predicted well?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "A linear regression model with all of the features led to these regression_error results:\n",
    "training: 13.910571985204234\n",
    "test: 14.171132427777215\n",
    "These numbers don't seem so bad for circumferences in the range of 502 - 653.\n",
    "\n",
    "The errors are pretty similar which is a good sign, but the model isn't great at predicting circumference values on the low and high end in both the training and test data, roughly above 600 and below 540. \n",
    "\n",
    "When adding additional features using PolynomialFeatures the regression_error reduced a bit in training to \n",
    "12.626836323124518\n",
    "but it still wasn't great at predicting the high and low values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
